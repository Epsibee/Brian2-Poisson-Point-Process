{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ced7c142-32a0-461f-b2f3-800814b5ab3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # used for array operations\n",
    "from scipy.stats import poisson, binom # used for stats (mean, s.d.)\n",
    "import brian2 as b2 # used for neural simulation\n",
    "from brian2 import prefs\n",
    "prefs.codegen.target = \"numpy\"\n",
    "import matplotlib.pyplot as plt # used for plotting\n",
    "\n",
    "# Simulates multiple independent Poisson processes for a group of neurons\n",
    "def independent_poisson_processes(num_neurons, rate, time, num_samples): \n",
    "    # Convert time to seconds to Brian2 units\n",
    "    simulation_time = time * b2.second\n",
    "    # Number of time bins (in milisecs) used to discretize\n",
    "    n_bins = int(time * 1000)\n",
    "    \n",
    "    #3D array to store spike trains of parameters\n",
    "    processes = np.zeros((num_samples, num_neurons, n_bins))\n",
    "\n",
    "    # tried to vectorize and use for-loops as least as possible, some remain\n",
    "    for sample in range(num_samples):\n",
    "        # Create Poisson neurons and monitor their spikes\n",
    "        poisson_group = b2.PoissonGroup(num_neurons, rate * b2.Hz) # represents the neurons created, firing at the specified rate with int and rate\n",
    "        spike_monitor = b2.SpikeMonitor(poisson_group) # records the spikes genreeated by the poissongroup with source\n",
    "        b2.run(simulation_time)  # time\n",
    "        # obtaining spike times for each neuron\n",
    "        spike_trains = spike_monitor.spike_trains() \n",
    "        \n",
    "        # Convert continuous spike times to discrete time bins\n",
    "        for neuron_idx in spike_trains:\n",
    "            spike_times = spike_trains[neuron_idx]\n",
    "            # Convert spike times from seconds to milisecond indices\n",
    "            spike_indices = [int(float(t/b2.ms)*10) for t in spike_times if t < simulation_time]\n",
    "            # Mark spike occurrences in the processes array\n",
    "            for idx in spike_indices:\n",
    "                if idx < n_bins:\n",
    "                    processes[sample, neuron_idx - 1, idx] = 1\n",
    "        \n",
    "        # Clean up Brian2 objects\n",
    "        net = None\n",
    "\n",
    "    # Analysis of spike patterns:\n",
    "    summing_variable = np.sum(processes, axis=1) # sum across neurons to get population activity\n",
    "    counts = np.sum(processes, axis=2) # count total spikes for each neuron in each sample\n",
    "    vector_set_of_counts = np.sum(counts, axis=1) # sum spike counts across neurons for each sample\n",
    "    characterization_of_counts = np.mean(vector_set_of_counts) # calculate average spike count across all samples\n",
    "    \n",
    "    # Find neurons with minimum spike count\n",
    "    least_count = np.min(counts)\n",
    "    least_count_indices = np.where(counts == least_count)\n",
    "    least_count_integers = np.arange(1, least_count + 1)\n",
    "    \n",
    "    return processes, summing_variable, counts, vector_set_of_counts, characterization_of_counts, least_count, least_count_indices, least_count_integers\n",
    "\n",
    "def correlated_poisson_processes(num_neurons, rate, time, num_common, num_samples):\n",
    "    # Create a network with independent Poisson neurons\n",
    "    # Each neuron is reprsented by a seperate PoissonGroup with rate\n",
    "    net = b2.Network()\n",
    "    poisson_groups = [b2.PoissonGroup(1, rate*b2.Hz) for _ in range((num_neurons + num_common) * num_samples)]\n",
    "    net.add(poisson_groups)\n",
    "    \n",
    "    # Setting up spike monitors to record the activity of each neuron with a simulation run\n",
    "    spike_monitors = [b2.SpikeMonitor(group) for group in poisson_groups]\n",
    "    net.add(spike_monitors)\n",
    "    net.run(time * b2.ms) # should all of the b2.ms be (t/b2.ms)*10) ????\n",
    "\n",
    "    # Convert spike trains for Brian2 format to numpy arrays, intialize storing process, convert spike times to discrete time indicies\n",
    "    spike_trains = [monitor.spike_trains()[0] for monitor in spike_monitors]\n",
    "    independent_procs = np.zeros((num_samples, num_neurons + num_common, int(time)), dtype=bool)\n",
    "    spike_indices_list = [[int(t / b2.ms) for t in spike_train if t < time * b2.ms] for spike_train in spike_trains]\n",
    "\n",
    "    # Ensuring all spike trains have the same length by padding with zeros\n",
    "    max_length = max(len(indices) for indices in spike_indices_list)\n",
    "    padded_spike_indices_list = [indices + [0] * (max_length - len(indices)) for indices in spike_indices_list]\n",
    "\n",
    "    # Convert padded lists to numpy array for efficient processing\n",
    "    spike_indices = np.array(padded_spike_indices_list)\n",
    "\n",
    "    # Calculate sample and neuron idicies for efficient array indexing\n",
    "    sample_indices, neuron_indices = np.divmod(np.arange((num_neurons + num_common) * num_samples), num_neurons + num_common)\n",
    "    for i, indices in enumerate(spike_indices):\n",
    "        for index in indices:\n",
    "            if index < int(time):\n",
    "                independent_procs[sample_indices[i], neuron_indices[i], index] = True\n",
    "\n",
    "    # Generate correlated processes:\n",
    "    correlated_procs = np.zeros((num_samples, num_neurons, int(time)), dtype=bool) #  by combining independent processes\n",
    "    correlated_procs[:, :num_common, :] = np.cumsum(independent_procs[:, :num_common, :], axis=2) # num_common neurons are correlated through cumsum\n",
    "    correlated_procs[:, num_common:, :] = independent_procs[:, num_common:num_neurons, :] # remaining neurons maintain their independent firing patterns\n",
    "\n",
    "    return correlated_procs # should this return more?\n",
    "\n",
    "def small_network(num_neurons, rate, time):\n",
    "    neurons = b2.NeuronGroup(num_neurons, 'dv/dt = -v/(10*ms) : 1', threshold='v>1', reset='v=0')\n",
    "    inputs = b2.PoissonInput(target=neurons, target_var='v', N=num_neurons, rate=rate*b2.Hz, weight=1)\n",
    "    spike_monitor = b2.SpikeMonitor(neurons)\n",
    "    net = b2.Network(neurons, inputs, spike_monitor)\n",
    "    net.run(time * b2.ms)  # Run the network instead of b2.run\n",
    "    return spike_monitor.spike_trains()\n",
    "\n",
    "# start - counting functions\n",
    "\n",
    "# The counting functions are designed to calculate the number of spikes in a neuron or a group of neurons over time.\n",
    "# These functions take in an array of spike counts and return the cumulative sum of the spike counts up to a specified time.\n",
    "# The count_at_time function calculates the cumulative sum of the spike counts up to a specified time t,\n",
    "# while the count1 function calculates the cumulative sum of the spike counts for a single neuron over time.\n",
    "# The countall function calculates the cumulative sum of the spike counts for all neurons over time,\n",
    "# and the counting_process_nd function calculates the cumulative sum of the spike counts for multiple neurons over time.\n",
    "\n",
    "def count_at_time(counts, times, t):\n",
    "    return np.cumsum(counts[:np.sum(times < t)])\n",
    "\n",
    "def count1(process, time):\n",
    "    # **Changed to use np.cumsum**\n",
    "    return np.cumsum(process)[:time]\n",
    "\n",
    "def countall(processes, time):\n",
    "    counts = []\n",
    "    for process in processes:\n",
    "        count = count1(process, time)\n",
    "        counts.append(count)\n",
    "    return counts\n",
    "\n",
    "def countall_vectorized(processes, time):\n",
    "    # **Added this function to vectorize the counting process**\n",
    "    counts = [np.cumsum(process) for process in processes[0]]\n",
    "    return counts\n",
    "\n",
    "def counting_process_nd(independent_processes, num_samples, time, num_neurons_to_plot):\n",
    "    counting_process_nd = [[[0 for _ in range(time)] for _ in range(num_neurons_to_plot)] for _ in range(num_samples)]\n",
    "    for i in range(num_samples):\n",
    "        counts = [0] * num_neurons_to_plot\n",
    "        for j in range(time):\n",
    "            for k in range(num_neurons_to_plot):\n",
    "                if independent_processes[i][k][j] == 1:\n",
    "                    counts[k] += 1\n",
    "            for k in range(num_neurons_to_plot):\n",
    "                counting_process_nd[i][k][j] = counts[k]\n",
    "    return counting_process_nd\n",
    "# end - counting functions\n",
    "\n",
    "# start - plot functions\n",
    "\n",
    "# The plotting functions are designed to visualize the spike counts and other data.\n",
    "# The plot_neurons_spiking function plots the spike counts for multiple neurons over time,\n",
    "# while the plot_neuron_spiking_standard_dev function plots the spike counts for a single neuron over time,\n",
    "# along with the mean and standard deviation of the spike counts.\n",
    "# The plot_two_neurons_against_time function plots the spike counts for two neurons over time,\n",
    "# along with the cumulative sum of the spike counts for a third neuron.\n",
    "\n",
    "def plot_neurons_spiking(processes, time): # showing some examples of neurons spiking\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    for i in range(len(processes[0])):\n",
    "        plt.plot(processes[0][i], label=f'Neuron {i}')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Spike')\n",
    "    plt.title('Neurons Over Time')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_neuron_spiking_standard_dev(processes, time):\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    plt.plot(processes[0][0], label='Neuron 1')\n",
    "    sd = np.std(processes[0][0])\n",
    "    mean = np.mean(processes[0][0])\n",
    "    print(f\"Mean: {mean:.2f}\")\n",
    "    print(f\"Standard Deviation: {sd:.2f}\")\n",
    "    plt.fill_between(range(time), processes[0][0] - sd, processes[0][0] + sd, alpha=0.2, label='Standard Deviation')\n",
    "    plt.axhline(y=mean, color='black', linestyle='--', label='Mean')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Spike')\n",
    "    plt.title('Neuron 1 Over Time with Standard Deviation')\n",
    "    plt.legend()\n",
    "    plt.text(0.5, 0.9, f\"Mean: {mean:.2f}, SD: {sd:.2f}\", transform=plt.gca().transAxes)\n",
    "    plt.show()\n",
    "\n",
    "def plot_two_neurons_against_time(processes, time):\n",
    "    fig = plt.figure(figsize=(10,6))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot(processes[0][0], processes[0][1], np.cumsum(processes[0][2]))\n",
    "    ax.set_xlabel('Neuron 1')\n",
    "    ax.set_ylabel('Neuron 2')\n",
    "    ax.set_zlabel('Cumulative Sum of Neuron 3')\n",
    "    plt.title('Two Neurons Over Time with Cumulative Sum of Third Neuron')\n",
    "    plt.show()\n",
    "\n",
    "def plot_count_neuron1_vs_time(counting_process_nd, num_samples):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for i in range(num_samples):\n",
    "        plt.plot(counting_process_nd[i][0], label=f'Sample {i}')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Count of Neuron 1')\n",
    "    plt.title('Count of Neuron 1 Over Time')\n",
    "    plt.ylim(0, None)  # Set y-axis lower limit to 0\n",
    "    plt.show()\n",
    "\n",
    "def plot_count_neuron1_vs_neuron2_vs_time(counting_process_nd, num_samples):\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    for i in range(num_samples):\n",
    "        ax.plot(counting_process_nd[i][0], counting_process_nd[i][1], range(len(counting_process_nd[i][0])))\n",
    "    ax.set_xlabel('Count of Neuron 1')\n",
    "    ax.set_ylabel('Count of Neuron 2')\n",
    "    ax.set_zlabel('Time', rotation=90)\n",
    "    ax.set_title('Count of Neuron 1 vs Count of Neuron 2 vs Time')\n",
    "    ax.set_xlim(0, max([max(counting_process_nd[i][0]) for i in range(num_samples)]))\n",
    "    ax.set_ylim(0, max([max(counting_process_nd[i][1]) for i in range(num_samples)]))\n",
    "    ax.set_zlim(0, max([len(counting_process_nd[i][0]) for i in range(num_samples)]))\n",
    "    plt.show()\n",
    "\n",
    "def plot_count1(count1_output, title='Count of Spikes Over Time (Single Neuron)'):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(count1_output)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    print(f\"Plot of {title} generated successfully.\")\n",
    "\n",
    "def plot_vectorized_count1(count1_vectorized_output, title='Count of Spikes Over Time (Single Neuron, Vectorized)'):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(count1_vectorized_output)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    print(f\"Plot of {title} generated successfully.\")\n",
    "\n",
    "def plot_counts(counts_all, title='Count of Spikes Over Time'):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for i, count in enumerate(counts_all):\n",
    "        plt.plot(count, label=f'Neuron {i}')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_counts_vectorized(counts_all_vectorized, title='Count of Spikes Over Time (Vectorized)'):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for i, count in enumerate(counts_all_vectorized):\n",
    "        plt.plot(count, label=f'Neuron {i}')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "# end - plot functions\n",
    "\n",
    "# start - stats functions\n",
    "\n",
    "# The statistical functions are designed to calculate statistical properties of the spike counts.\n",
    "# These functions take in an array of spike counts and return statistical properties such as the mean, covariance, and slope of the spike counts. \n",
    "# The calculate_mean_and_covariance function calculates the mean and covariance of the spike counts for multiple neurons\n",
    "# over time, while the get_slope function calculates the slope of the spike counts for multiple neurons over time.\n",
    "# The calculate_mean_with_std function calculates the mean and standard deviation of the spike counts for a single neuron over time,\n",
    "# and the calculate_covariance_matrix function calculates the covariance matrix of the spike counts for multiple neurons over time.\n",
    "\n",
    "def calculate_mean_and_covariance(processes):\n",
    "    means = []\n",
    "    covariances = []\n",
    "    for i in range(len(processes[0])):\n",
    "        mean = np.mean([process[i] for process in processes])\n",
    "        covariance = np.cov([process[i] for process in processes], rowvar=False)\n",
    "        means.append(mean)\n",
    "        covariances.append(covariance)\n",
    "    return means, covariances\n",
    "\n",
    "def get_slope(processes):\n",
    "    slopes = []\n",
    "    for i in range(len(processes[0])):\n",
    "        slope = np.polyfit(range(len(processes[0][i])), processes[0][i], 1)[0]\n",
    "        slopes.append(slope)\n",
    "    return slopes\n",
    "\n",
    "def calculate_mean_with_std(processes):\n",
    "    mean_with_std = np.mean(processes[0][0]) + np.std(processes[0][0])\n",
    "    return mean_with_std\n",
    "\n",
    "def calculate_covariance_matrix(processes):\n",
    "    cov_matrix = np.cov([processes[0][0], processes[0][1]])\n",
    "    return cov_matrix\n",
    "# end - stats functions\n",
    "\n",
    "# start -  Discretize time and sampling functions\n",
    "\n",
    "# The discretization and sampling functions are designed to discretize the time values into bins of a specified size and sample\n",
    "# the spike counts at a specified resolution. The discretize_time function discretizes the time values into bins of a specified size,\n",
    "# while the sample_spikes function samples the spike counts at a specified resolution.\n",
    "# These functions are useful for analyzing the spike counts at different time scales.\n",
    "\n",
    "def discretize_time(processes, time):\n",
    "    discretized_processes = []\n",
    "    for process in processes:\n",
    "        discretized_process = np.array([process[i] for i in range(0, len(process), time)])\n",
    "        discretized_processes.append(discretized_process)\n",
    "    return discretized_processes\n",
    "\n",
    "def sample_spikes(processes, resolution):\n",
    "    sampled_processes = []\n",
    "    for process in processes:\n",
    "        sampled_process = process[::resolution]\n",
    "        sampled_processes.append(sampled_process)\n",
    "    return sampled_processes\n",
    "# end -  Discretize time and sampling function\n",
    "\n",
    "# start - tagging and branding functions\n",
    "\n",
    "# The tagging and branding functions are designed to tag each event in the spike counts with a Poisson process and simulate\n",
    "# a network of neurons. The tag_events_with_poisson function tags each event in the spike counts with a Poisson process,\n",
    "# while the brand_networks function simulates a network of neurons with a specified number of neurons, firing rate, and time,\n",
    "# and returns the spike trains of the neurons. These functions are useful for analyzing the behavior of neural networks.\n",
    "\n",
    "\n",
    "def tag_events_with_poisson(processes, rate):\n",
    "    if isinstance(processes, (np.float64, float, int)):\n",
    "        # If single value, return single Poisson sample\n",
    "        return np.random.poisson(rate)\n",
    "    else:\n",
    "        # If array, process as before\n",
    "        tagged_processes = []\n",
    "        for process in processes:\n",
    "            tagged_process = np.random.poisson(rate, size=len(process))\n",
    "            tagged_processes.append(tagged_process)\n",
    "        return tagged_processes  \n",
    "\n",
    "def brand_networks(num_neurons, firing_rate, time, num_rates=None):\n",
    "    neurons = b2.NeuronGroup(num_neurons, 'dv/dt = -v/(10*ms) : 1', threshold='v>1', reset='v=0')\n",
    "\n",
    "    if num_rates is None:\n",
    "        # Use a single rate for all neurons\n",
    "        inputs = b2.PoissonInput(target=neurons, target_var='v', N=num_neurons, rate=firing_rate*b2.Hz, weight=1)\n",
    "    else:\n",
    "        # Use multiple rates for different neurons\n",
    "        input_rates = np.random.uniform(0, firing_rate, size=num_rates)\n",
    "        inputs = []\n",
    "        for i in range(num_neurons):\n",
    "            input_rate = input_rates[i % num_rates] * b2.Hz\n",
    "            input_ = b2.PoissonInput(target=neurons, target_var='v', N=1, rate=input_rate, weight=1)\n",
    "            inputs.append(input_)\n",
    "\n",
    "    spike_monitor = b2.SpikeMonitor(neurons)\n",
    "    net = b2.Network(neurons, inputs, spike_monitor)\n",
    "    net.run(time * b2.ms)  # Run the network instead of b2.run\n",
    "    return spike_monitor.spike_trains() \n",
    "# end - tagging and branding functions\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:brian_tutorial]",
   "language": "python",
   "name": "conda-env-brian_tutorial-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
